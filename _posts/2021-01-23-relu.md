---
title: "ReLU"
permalink: /blog/tf_04/
date: 2021-01-23 11:34
background:

categories:
  - blog
tags:
  - blog
  - python
  - tensorflow
  - deeplearning
last_modified_at: 2021-01-18T11:37:53+09:00
---

Lecture from 

[텐서플로우로 시작하는 딥러닝 기초](https://www.boostcourse.org/ai212)

---

# Problem with sigmoid

gradient close to 0 at limits

gradient very big around x=0 (middle)

when network is deep, multiple functions combined, gradient is 'vanishing' (multiply small numbers will result 0)

# ReLU

<img src="https://render.githubusercontent.com/render/math?math=\displaystyle f(x)=max(0,x)">

![https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile1.uf.tistory.com%2Fimage%2F2238DC3558D62AF732F837](https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile1.uf.tistory.com%2Fimage%2F2238DC3558D62AF732F837)

![eq_relu.png](/assets/images/posts/2021-01-23/eq_relu.png)

Gradient will be well transferred even when the network is deep

limitation when x<0 <leaky relu can solve>

```python
tf.keras.activations : sigmoid, tanh, relu, elum, selu
tf.keras.layers : leaky relu
```

## Xavier Initialization

<img src="https://render.githubusercontent.com/render/math?math=var=\frac2{channel\_in+channel\_out}">

- channel in & out: number if inputs/outputs

```python
tf.keras.initializers.glorot_uniform()
```

## He Initialization

best works for ReLU

<img src="https://render.githubusercontent.com/render/math?math=var=\frac4{channel\_in+channel\_out}">

```python
tf.keras.initializers.he_uniform()
```