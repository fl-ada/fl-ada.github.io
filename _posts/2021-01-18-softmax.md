---
title: "Softmax"
permalink: /blog/tf_03/
date: 2021-01-18 14:12
background:

categories:
  - blog
tags:
  - blog
  - python
  - tensorflow
  - deeplearning
last_modified_at: 2021-01-18T14:14:51+09:00
---

Lecture from 

[텐서플로우로 시작하는 딥러닝 기초](https://www.boostcourse.org/ai212)

---

# Softmax

Generalized logistic regression for **Multinomial Classification**

- run n logistic regressions where n is no. of classification groups
- input: result of logistic regressions for all groups
- output: probability of fitting in each group
    - <img src="https://render.githubusercontent.com/render/math?math=\sum=1">
- based on the probability, one-hot encoding to choose the most likely group

## Cross entropy (loss)

<img src="https://render.githubusercontent.com/render/math?math=-\sum (y\cdot \log\hat y)$$">

∴ for the correct label y=1,

<img src="https://render.githubusercontent.com/render/math?math=\hat y \approx 1"> gives small cross-entropy <correct prediction>

while <img src="https://render.githubusercontent.com/render/math?math=\hat y \approx 0"> gives very big cross-entropy <wrong prediction>

# Code in TF

## Initialization

```python
W = tf.Variable(tf.random.normal((x_cols, y_cols)), name='weight')
b = tf.Variable(tf.random.normal((y_cols,)), name='bias')
variables = [W,b]
```

## Build model

### hypothesis

<img src="https://render.githubusercontent.com/render/math?math=\displaystyleH(X)=XW+b">

```python
def hypo(X):
	return tf.nn.softmax(tf.matmul(X,W) + b)
```

### cross-entropy (loss function)

<img src="https://render.githubusercontent.com/render/math?math=\displaystyle\xi(X,Y)=-\sum y_i\log\hat y">

```python
def cost_func(X,Y):
	cost = -tf.reduce_sum(Y * tf.math.log(hypo(X)), axis = 1)
	cost_mean = tf.reduce_mean(cost
	return cost_mean
```

### gradient descent

```python
def grad_func(X,Y):
	with tf.GradientTape() as tape:
		loss = cost_func(X,Y)
		grads = tape.gradient(loss, variables)
	return grads
```

## Fit model

```python
def fit(X,Y,epochs, verbose):
	optimizer = tf.keras.optimizers.SGD(learning_rate)

	for i in range(epochs):
		grads = grad_func(X,Y)
		optimizer.apply_gradients(zip(grads, variables))
		# optional, print intermediate results
		if (i==0) | (i+1) % verbose == 0):
			print('Loss at epoch %d : %f' %(i+1, cost_func(X,Y).numpy()))
```

## Convert as Class

- combine all codes above

    ```python
    class softmax_classifer(tf.keras.Model):
        def __init__(self, nb_classes):
            super(softmax_classifer, self).__init__()
            self.W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')
            self.b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')
            
        def softmax_regression(self, X):
            return tf.nn.softmax(tf.matmul(X, self.W) + self.b)
        
        def cost_fn(self, X, Y):
            logits = self.softmax_regression(X)
            cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(logits), axis=1))        
            return cost
        
        def grad_fn(self, X, Y):
            with tf.GradientTape() as tape:
                cost = self.cost_fn(x_data, y_data)
                grads = tape.gradient(cost, self.variables)            
                return grads
        
        def fit(self, X, Y, epochs=2000, verbose=500):
            optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)

            for i in range(epochs):
                grads = self.grad_fn(X, Y)
                optimizer.apply_gradients(zip(grads, self.variables))
                if (i==0) | ((i+1)%verbose==0):
                    print('Loss at epoch %d: %f' %(i+1, self.cost_fn(X, Y).numpy()))
                
    model = softmax_classifer(nb_classes)
    model.fit(x_data, y_data)
    ```