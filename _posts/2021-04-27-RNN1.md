---
title: "RNN(1)"
permalink: /blog/tf_07/
date: 2021-04-27 23:14
background:

categories:
  - blog
tags:
  - blog
  - python
  - tensorflow
  - deeplearning
last_modified_at: 2021-04-27T23:15:23+09:00
---

Lecture from : [텐서플로우로 시작하는 딥러닝 기초](http://www.boostcourse.org/ai212)

diagrams from : [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

---

# Recurrent Neural Network

- NN model for sequence data that NN and CNN cannot work on
    - language
        - language modeling : predict what word/character comes next
        - Translation
        - sentiment analysis, etc.
    - image/video

![RNN_structure.png](/assets/images/posts/2021-04-27/RNN_structure.png)

- $A$ : same function using same parameters for every recurrent step

![RNN_eq1.png](/assets/images/posts/2021-04-27/RNN_eq1.png)

<img src="https://render.githubusercontent.com/render/math?math=Y_t=W_{hy}h_t">

## @TensorFlow

`?` : parameters to be set

### Load libraries

```python
import numpy as np

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import Sequential, Model
from tensorflow.keras.preprocessing.sequence import pad_sequences
```

### Prepare dataset

- data shape:
    - `x_data : (batch_size, sequence_length, input_dimension)`
    - `h_t : (batch_size, sequence_length, hidden_size)`
- Create dictionary for one-hot encoding

    ```python
    words = [x1, x2, ...]

    # dictionary
    char_set = ['<pad>'] + sorted(list(set('',join(words))))
    idx2char = {idx : char for idx, char in enumerate(char_set)}
    char2idx = {char : idx for idx, char in enumerate(char_set)}

    # one-hot encoding
    x_data = list(map(lambda word : [char2idx.get(char) for char in word], words)

    # padding
    x_data_len = list(map(lambda word: len(word), x_data))
    max_sequence = ?
    x_data = pad_sequences(sequences = x_data, maxlen = max_sequence, padding= 'post', truncating = 'post')
    ```

### Build model

Vanilla RNN

1. build cell first, then put into NN model

    ```python
    hidden_size = ?
    cell = layers.SimpleRNNCell(units = hidden_size)
    rnn = layers.RNN(cell, return_sequences=True, return_state=True)
    outputs, states = rnn(x_data)
    # outputs : h_t matrix
    # states : last output
    ```

2. directly build NN model

    ```python
    rnn = layers.SimpleRnn(units = hidden_size, return_sequences = True, return_state = True)
    outputs, states = rnn(x_data)
    ```

Sequential model

```python
input_dim = len(char2idx) # no. of characters in dic including <pad>
output_dim = len(char2idx)
one_hot = np.eye(len(char2idx))
hidden_size = ?
num_classes = ?

model = Sequential()
model.add(layers.Embedding(input_dim = input_dim, output_dim = output_dim, trainable=False, mask_zero=True, input_length=max_sequence,
													embeddings_initializer=keras.initializers.Constant(one_hot)))
# trainable : train one-hot vector
# mask_zero : ignore <pad>
model.add(layers.SimpleRNN(units=hidden_size)
model.add(layers.Dense(units=num_classes)) # many-to-one
```

### Train model

- loss function

    ```python
    def loss_fn(model, x, y):
    	return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=model(x), from_logits=True))
    ```

- optimizer

    ```python
    learning_rate = ?
    epochs = ?
    batch_size = ?
    opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)
    ```

- data pipeline

    ```python
    tr_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))
    tr_dataset = tr_dataset.shuffle(buffer_size = ?)
    tr_dataset = tr.dataset.batch(batch_size = batch_size)
    ```

- train

    ```python
    tr_loss_hist = [] # to show loss change

    for epoch in range(epochs):
    	avg_tr_loss = 0
    	tr_step = 0

    	for x_mb, y_mb in tr_dataset:
    		with tf.GradientTape() as tape:
    			tr_loss = loss_fn(model, x=x_mb, y=y_mb)
    		grads = tape.gradient(target=tr_loss, sources=model.variables)
    		opt.apply_gradients(grads_and_vars=zip(grads, model.variables))
    		avg_tr_loss += tr_loss
    		tr_step += 1
    	else:
    		avg_tr_loss /= tr_step
    		tr_loss_hist.append(avg_tr_loss)

    	# to see change in train_loss by epoch
    	if (epoch + 1) % 5 ==0:
    		print('epoch : {:3}, tr_loss : {:.3f}'.format(epoch + 1, avg_tr_loss.numpy()))
    ```

### Check accuracy

```python
yhat = model.predict(x_data)
yhat = np.argmax(yhat, axis=-1)
print('acc : {:.2%}'.format(np.mean(yhat == y_data)))
```